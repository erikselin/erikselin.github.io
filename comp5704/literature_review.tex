
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}
%--------------- Additiona packages -----------------------------------------
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{positioning}
\tikzset{font=\scriptsize}
% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: XRT 0.1.0 - A Language-Agnostic MapReduce Runtime for
Shared-Memory Systems}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Erik Selin\\
School of Information Technology and Engineering\\
University of Ottawa\\
Ottawa, Canada K1N 6N5 \\
{\em erik.selin@gmail.com}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle

% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

MapReduce is a restrictive programming model used to write efficient and highly
parallel data processing programs without having to deal with the complexities
of parallel programming. The programming model was initially introduced by
Google in 2004 \cite{GoogleMapReduce} and later popularized by Yahoo! through
the Apache open-source project Hadoop MapReduce \cite{Hadoop}. Hadoop MapReduce
provides a MapReduce runtime for networked commodity hardware and its popularity
has resulted in the development of multiple runtimes for alternative
environments like GPUâ€™s \cite{Mars}, FGPA's \cite{Melia}, Coprocessors
\cite{MrPhi} and shared-memory systems \cite{Phoenix} \cite{Phoenix++}
\cite{CilkMR} \cite{Metis} \cite{Ostrich}.

The increasing availability of cost-effective shared-memory systems with high
core counts \cite{AWS} \cite{GoogleCloud} means that shared-memory systems are
becoming a serious alternative to networked commodity hardware for parallel data
processing. From a previous survey of the literature it was observed that
MapReduce runtimes for shared-memory systems are all built with the sole purpose
of beating benchmarks. They tend to require the programmer to write mapper and
reducer code in C or C++, are not cross platform compatible, occasionally break
the MapReduce abstraction and in some cases are not even available for use.

The purpose of this literature review is to confirm these observations as well
as dig deeper into the literature to paint a clear picture of the current state
of MapReduce runtimes for shared-memory systems. Ultimately, the goal is to
justify the creation of XRT, a new cross-platform language-agnostic MapReduce
runtime for shared-memory systems that is inspired by Hadoop Streaming
\cite{HadoopStreaming} and based on techniques developed in modern MapReduce
runtimes for shared-memory systems \cite{Phoenix} \cite{Phoenix++} \cite{CilkMR}
\cite{Ostrich} \cite{Metis}.

% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

\subsection{The MapReduce programming model}

\input{figure-mapreduce.tex}

In the classical MapReduce programming model \cite{GoogleMapReduce}, illustrated
in Figure \ref{fig:mapreduce}, programmers only need to supply mapper and
reducer programs together with the source of the input and the destination of
the output, everything else is handled by the runtime. For most MapReduce
runtimes this means executing parallel workers running multiple instances of the
mapper and reducer programs, reading and distribution of the input, collection
and writing of the output and shuffling and sorting intermediate data. A
classical MapReduce job starts by splitting the input and running a mapper for
each split. Each mapper will consume the records in the split and output
key-value pairs which are collected by the runtime and partitioned into sorted
buckets. Key-value pairs with the same key end up in the same bucket no matter
which mapper produced the pair. Once all mappers are done a reducer is started
for each bucket and all key-value pairs in the bucket are processed in order.
This means that all key-value pairs produced by the mappers with the same key
will all be processed by the same reducer. Finally, the output of the reducers
becomes the output of the job.

There is a lot of room for implementation specific deviations and optimizations
in this description but on a high level all MapReduce runtimes operates by this
model \cite{GoogleMapReduce} \cite{Hadoop} \cite{Phoenix} \cite{Phoenix++}
\cite{CilkMR} \cite{Metis} \cite{Ostrich}. The success of the MapReduce model
comes from the ease of writing sequential mapper and reducer programs that are
able to accomplish advanced data transformation tasks in parallel when executed
through a MapReduce runtime \cite{GoogleMapReduce}.

\subsection{MapReduce language support}

MapReduce runtimes with multi-language or language-agnostic support is not
something that has been greatly explored by the MapReduce community. In general
MapReduce runtimes are single language: Googles proprietary MapReduce is
described as a C++ runtime \cite{GoogleMapReduce}, Hadoop MapReduce provides a
runtime for Java \cite{Hadoop} and shared-memory runtimes like Phoenix
\cite{Phoenix}, Phoenix++ \cite{Phoenix++}, Metis \cite{Metis}, Ostrich
\cite{Ostrich} and CilkMR \cite{CilkMR} provide runtimes for C or C++.

In fact, the only available language-agnostic MapReduce runtime seems to be
Hadoop Streaming. Hadoop Streaming is built upon Hadoop MapReduce and achieves
language-agnostic support by replacing the mappers and reducers with externally
run processes that communicates with the MapReduce runtime over the standard
streams \cite{HadoopStreaming}. Unfortunately the MapReduce runtime that power
Hadoop Streaming is still the Java optimized Hadoop MapReduce runtime and the
resulting performance of Hadoop Streaming is very bad \cite{HadoopStreamingPerf}
\cite{Pydoop} \cite{Perldoop}. ShmStreaming is an attempt to increase the
performance of Hadoop Streaming by communicating over shared memory instead of
the standard streams \cite{ShmStreaming}. The performance of Hadoop Streaming +
ShmStreaming is superior to Hadoop Streaming but communication over shared
memory is not as straightforward to implement compared to communication over
standard streams. In addition, ShmStreaming still suffers from the fact that
Hadoop MapReduce is not optimized for interacting with mappers and reducers that
are run as external processes.

The Hadoop community has also developed Hadoop Pipes which outperforms Hadoop
Streaming but only offers support for C++ \cite{HadoopPipes}. Language support
for the python programming language without using Hadoop Streaming is available
through the Pydoop project. Pydoop integrates with Hadoop MapReduce by wrapping
Hadoop Pipes and offers superior performance versus running python mapper and
reducers through Hadoop Streaming \cite{Pydoop}. An alternative approach to
adding specific language support to Hadoop MapReduce was undertaken in the
Perldoop project \cite{Perldoop}. Perldoop provides a perl-to-java transpiler
specifically built to convert Perl Hadoop MapReduce jobs to Java Hadoop
MapReduce jobs. Perldoop can achieve very good performance since it is
effectively running regular Java Hadoop MapReduce however there are a lot of
limitations since only a subset of very specifically formatted Perl is
supported.

Finally, there seems to be no MapReduce runtime that is built explicitly for
multi-language or truly language-agnostic MapReduce.

\subsection{MapReduce on shared-memory systems}

Shared-memory systems are becoming increasingly capable and cost effective for
data processing. At the time of writing it is possible to get a on demand system
with 128 cores and 3.9 terabytes of memory for \$26/hour \cite{AWS}. Based on
recent releases from popular cloud providers the trend of affordable high
core-count systems is likely to continue and cost per core will likely keep
decreasing \cite{AWS} \cite{GoogleCloud}.

\input{figure-memorylayout.tex}

MapReduce on shared-memory systems was first explored by the Phoenix
\cite{Phoenix} project. Besides being first to explore MapReduce on
shared-memory systems the Phoenix project also contributed the matrix
memory-layout illustrated in Figure \ref{fig:memorylayout}. This memory layout
is used in some form by all shared-memory runtimes \cite{Phoenix}
\cite{Phoenix++} \cite{CilkMR} \cite{Metis} \cite{Ostrich} and enables workers
to communicate extremely efficiently with the runtime while avoiding contention.
The columns in the matrix represent partitions and each entry in the matrix is a
buffer responsible for storing the intermediate data produced by a mapper.
During the mapper stage each mapper is given access to a row in the matrix and
during the reducer stage each reducer is given access to a column in the matrix.
This methodology ensures that no communication is required across workers during
execution which in turns maximizes throughput.

The Phoenix runtime has gone through multiple iterations and Phoenix 2.0 was the
result of performance issues when Phoenix was run on larger shared-memory
systems processing larger data sets \cite{Phoenix2}. In particular the
intermediate data buffers that are used to store keys from the key-value pairs
produced by the mappers are implemented as sorted arrays in Phoenix. This lead
to performance issues since the arrays needed to reallocate whenever they ran
out of space and whenever a new key was added all keys coming after the new key
had to be shifted. Phoenix 2.0 solved this issue by significantly increasing the
number of sorted key arrays so that on average only one key resides in each
array \cite{Phoenix2}.

Further work on the Phoenix project has lead to the determination that the
intermediate sorted arrays were limiting the potential performance of the
runtime. This resulted in the reimplementation of Phoenix in C++ and the start
of the Phoenix++ project \cite{Phoenix++}. Phoenix++ is the poster child of
shared-memory MapReduce runtimes optimized for speed and extends the MapReduce
API to include containers and combiner objects. Containers exposes the internals
of the shuffle stage to the programmer and makes it possible to select or tune
the data structure used for shuffling \cite{Phoenix++}. Combiner objects exposes
the internals of the intermediate data buffers and makes it possible to select
the data structure used for intermediate data buffering \cite{Phoenix++}. While
the introduction of containers and combiner objects in Phoenix++ does allow for
better performance it has been argued that they break the MapReduce abstraction
and programmers now need to be intimately familiar with the Phoenix++ internals
to pick the correct containers and combiner objects \cite{CilkMR}.

An alternative approach to dealing with the perceived shortcomings of the
Phoenix project is the Metis project. Instead of increasing the configurability
of the runtime Metis introduces a compromise intermediate data buffer
\cite{Metis}. Using a more advanced buffer Metis is able to achieve significant
performance increase over Phoenix on data processing problems involving very
little mapper/reducer computation on datasets with few keys but many duplicates.
Metis does not offer any performance increase over Phoenix on data processing
jobs involving a significant amount of mapper/reducer computation or on datasets
with few duplicate keys \cite{Metis}.

An extension of the Phoenix model is Tiled-MapReduce and its prototype
implementation Ostrich. Ostrich extends the model introduced by Phoenix by using
the tiling strategy commonly used in the compiler community. Ostrich and
Tiled-MapReduce runs splits the input into subsets and runs smaller jobs on each
subset. Each smaller job runs mappers that produces a intermediate data buffer
like Phoenix but then reduces the partial intermediate data buffer using a
combiner into a secondary intermediate data buffer. Once all smaller jobs have
run the secondary intermediate data buffers are reduced by the reducers. Since
the combiner reduces the amount of intermediate data this approach increases the
performance of the runtime and allows for processing of more data. In addition
the primary intermediate data buffer that is filled by the mappers can be reused
between the smaller jobs. This means that Ostrich is able to avoid a lot of
memory allocation calls compared to Phoenix and other shared-memory runtimes
\cite{Ostrich}.

The CilkMR project is one of the newest MapReduce runtimes for shared-memory
systems and addresses the key-value pair serialization/deserialization overhead
of all the previously discussed runtimes \cite{CilkMR}. Instead of operating on
key-value pairs CilkMR operates over typed data containers which means that the
mappers can pass arbitrary data structures to the reducers. CilkMR is also
inspired by Phoenix++ in that it allows the programmers to control intermediate
data structures and tune the runtime. The performance of CilkMR is really good
on computationally heavy tasks since mappers and reducers can operate on complex
data structures. However, Phoenix++ performs better than CilkMR on classical
data processing jobs like wc and strmatch \cite{CilkMR} \cite{GoogleMapReduce}.

A completely different idea for bringing MapReduce to shared-memory systems is
the Hone project. Hone attempts to take advantage of the familiarity of Hadoop
MapReduce by providing a Hadoop MapReduce compatible Java API
\cite{ScalingDown}. The goal is to create a a runtime that allow you to run the
exact same Java code that was written for Hadoop MapReduce. In benchmarks Hone
beats Phoenix however Phoenix++ beats Phoenix by a much wider margin and thus it
seems like Phoenix++ is likely much faster then Hone.

A last note about recent development in shared-memory MapReduce runtimes is the
usage of disk based data structures. In Hadoop MapReduce almost all intermediate
data will reside on disk at some point but in all the above mentioned
shared-memory runtimes there is never a mention of disk backing. There has been
some exploration in extending Metis to include the capacity of spilling
intermediate data to disk if the input is too large \cite{DiskOptimization}.
However, this optimization was never contributed back to the Metis project.

% ############################################################################
\section{Conclusion} \label{conclusion}
% ############################################################################

The field of shared-memory MapReduce runtimes is still extremely young and
almost all efforts are focused on beating benchmarks. Programming mapper and
reducer code for shared-memory MapReduce runtimes requires knowledge of C or C++
and sometimes a deep understanding of the runtime internals. A surprising
discovery from this review is that the shared-memory MapReduce community is not
really considering handling data that is too large to fit in memory.

Following the literature review the case for building XRT has been strengthen.
Shared-memory MapReduce runtimes lack a cross platform, disk aware and simple to
use runtime and the broader MapReduce community lack exploration of language
agnostic runtimes. That being said a lot of great ideas have already been
developed by the shared-memory MapReduce runtimes. Concepts like the matrix
memory layout, intermediate data structure reuse and language-agnostic support
through communication over standard streams will all be brough into XRT.

% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{bibliography}     %loads bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
